{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# BTS/WHITED Dataset - TSFM Embedding Generation\n\nThis notebook generates embeddings for both the BTS (Brick by Brick 2024) and WHITED datasets using time series foundation models (MOMENT and Chronos).\n\n**Two workflows are demonstrated:**\n1. **BTS Dataset:** Multi-label classification from building system sensor data (using MOMENT)\n2. **WHITED Dataset:** Single-label appliance classification from electrical power signatures (using MOMENT and Chronos)"
  },
  {
   "cell_type": "markdown",
   "id": "header-setup",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport pickle\nimport zipfile\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.interpolate import interp1d\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random\nimport soundfile as sf  # For WHITED FLAC files\nimport os\n\n# Note: Foundation model imports are done in their respective sections:\n# - momentfm (MOMENT model)\n# - chronos (Chronos model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-paths",
   "metadata": {},
   "outputs": [],
   "source": "# File paths for BTS dataset\nzip_file_path = 'data/train_X_v0.1.0.zip'\ntrain_y_path = 'data/train_y_v0.1.0.csv'\n\n# WHITED file path is set later in the WHITED section"
  },
  {
   "cell_type": "markdown",
   "id": "header-labels",
   "metadata": {},
   "source": "## BTS Label Mapping\n\nLoad and inspect the 94 label categories from the BTS dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and create mapping\n",
    "df_train_y = pd.read_csv(train_y_path, index_col=0)\n",
    "\n",
    "# Create label mapping\n",
    "label_mapping = {idx: label for idx, label in enumerate(df_train_y.columns)}\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "print(f\"\\nTotal number of label categories: {len(label_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-data-loading",
   "metadata": {},
   "source": "## BTS Data Loading and Processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_time_series(time_series, target_length=512):\n",
    "    \"\"\"\n",
    "    Resamples a time series to the target length using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: Original time series values\n",
    "    - target_length: Desired length for the resampled time series\n",
    "\n",
    "    Returns:\n",
    "    - Resampled time series of length target_length\n",
    "    \"\"\"\n",
    "    original_length = len(time_series)\n",
    "    if original_length == target_length:\n",
    "        return time_series\n",
    "    original_indices = np.linspace(0, 1, original_length)\n",
    "    target_indices = np.linspace(0, 1, target_length)\n",
    "    interpolator = interp1d(original_indices, time_series, kind='linear', fill_value=\"extrapolate\")\n",
    "    return interpolator(target_indices)\n",
    "\n",
    "\n",
    "def load_train_data(train_y_path, zip_file_path, seq_len=512, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Loads and resamples training data, then splits it into train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_y_path: Path to the training target CSV file\n",
    "    - zip_file_path: Path to the ZIP file containing the training .pkl files\n",
    "    - seq_len: Desired sequence length for resampling\n",
    "    - test_size: Proportion of the dataset to include in the test split\n",
    "    - random_state: Seed for reproducibility of the split\n",
    "\n",
    "    Returns:\n",
    "    - train_data, test_data: Train and test datasets with resampled timeseries and associated labels\n",
    "    \"\"\"\n",
    "    df_train_y = pd.read_csv(train_y_path, index_col=0)\n",
    "    print(f\"Loaded training labels. Number of samples: {len(df_train_y)}\")\n",
    "\n",
    "    resampled_data_list = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        pkl_files = zip_ref.namelist()\n",
    "        print(f\"Number of .pkl files in ZIP: {len(pkl_files)}\")\n",
    "\n",
    "        for idx, row in df_train_y.iterrows():\n",
    "            filename = row.name\n",
    "            if filename.endswith('.pkl'):\n",
    "                pkl_file = f\"train_X/{filename}\"\n",
    "            else:\n",
    "                pkl_file = f\"train_X/{filename}.pkl\"\n",
    "\n",
    "            labels = row.values\n",
    "\n",
    "            if pkl_file in pkl_files:\n",
    "                with zip_ref.open(pkl_file, 'r') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    resampled_values = resample_time_series(data['v'], target_length=seq_len)\n",
    "                    resampled_data_list.append({\n",
    "                        \"timeseries\": resampled_values,\n",
    "                        \"labels\": labels\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"File not found in ZIP: {pkl_file}\")\n",
    "\n",
    "    print(f\"Number of resampled samples: {len(resampled_data_list)}\")\n",
    "\n",
    "    if len(resampled_data_list) == 0:\n",
    "        raise ValueError(\"No matching files found between the labels and ZIP contents.\")\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(\n",
    "        resampled_data_list, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "class ClassificationDatasetWithMask(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for multi-label classification with MOMENT model.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, seq_len=512):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - data_list: List of dictionaries containing \"timeseries\" and \"labels\"\n",
    "        - seq_len: Fixed sequence length for each time series\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "        self.input_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for entry in data_list:\n",
    "            timeseries = entry[\"timeseries\"]\n",
    "            labels = entry[\"labels\"]\n",
    "\n",
    "            # Add channel dimension (1 for single-channel data)\n",
    "            timeseries = np.expand_dims(timeseries, axis=0)\n",
    "\n",
    "            # Create input mask\n",
    "            input_mask = np.ones(seq_len)\n",
    "\n",
    "            # Append to dataset\n",
    "            self.data.append(timeseries)\n",
    "            self.input_masks.append(input_mask)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "        # Convert lists to NumPy arrays\n",
    "        self.data = np.array(self.data)\n",
    "        self.input_masks = np.array(self.input_masks)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - Tuple of (timeseries, input_mask, label)\n",
    "        \"\"\"\n",
    "        timeseries = self.data[idx]\n",
    "        input_mask = self.input_masks[idx]\n",
    "        label = self.labels[idx]\n",
    "        return (\n",
    "            torch.tensor(timeseries, dtype=torch.float32),  # Shape: (1, seq_len)\n",
    "            torch.tensor(input_mask, dtype=torch.float32),  # Shape: (seq_len,)\n",
    "            torch.tensor(label, dtype=torch.float32),       # Shape: (num_labels,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-load-data",
   "metadata": {},
   "source": "## BTS: Load and Prepare Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with 80/20 train/test split\n",
    "train_data, test_data = load_train_data(train_y_path, zip_file_path, seq_len=512)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = ClassificationDatasetWithMask(train_data)\n",
    "test_dataset = ClassificationDatasetWithMask(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "# Verify data format\n",
    "for batch_x, batch_masks, batch_labels in train_dataloader:\n",
    "    print(f\"Batch X Shape: {batch_x.shape}\")  # Should be (batch_size, 1, seq_len)\n",
    "    print(f\"Batch Masks Shape: {batch_masks.shape}\")  # Should be (batch_size, seq_len)\n",
    "    print(f\"Batch Labels Shape: {batch_labels.shape}\")  # Should be (batch_size, num_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-visualization",
   "metadata": {},
   "source": "## Optional: Visualize BTS Sample Time Series"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_timeseries(dataset, dataset_name=\"Dataset\", label_mapping=None):\n",
    "    \"\"\"\n",
    "    Plots a random time series from the dataset and prints its labels.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: ClassificationDatasetWithMask dataset\n",
    "    - dataset_name: Name for the plot title\n",
    "    - label_mapping: Mapping of label indices to tag names\n",
    "    \"\"\"\n",
    "    random_idx = random.randint(0, len(dataset) - 1)\n",
    "    timeseries, _, labels = dataset[random_idx]\n",
    "\n",
    "    # Convert labels to numpy and find active labels\n",
    "    labels = labels.numpy()\n",
    "    active_label_indices = [idx for idx, value in enumerate(labels) if value > 0]\n",
    "    active_labels = [label_mapping[idx] for idx in active_label_indices] if label_mapping else active_label_indices\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(timeseries.squeeze(), label=\"Timeseries\")\n",
    "    plt.title(f\"Random Time Series from {dataset_name}\\nLabels: {', '.join(active_labels)}\")\n",
    "    plt.xlabel(\"Timestamps\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Labels for the selected time series from {dataset_name}: {active_labels}\")\n",
    "\n",
    "\n",
    "# Plot random samples\n",
    "plot_random_timeseries(train_dataset, dataset_name=\"Training Dataset\", label_mapping=label_mapping)\n",
    "plot_random_timeseries(test_dataset, dataset_name=\"Testing Dataset\", label_mapping=label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-embedding",
   "metadata": {},
   "source": "## MOMENT Embedding Generation (Used for Both Datasets)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, dataloader):\n",
    "    \"\"\"\n",
    "    Generates embeddings using MOMENT model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: MOMENT model in embedding mode\n",
    "    - dataloader: DataLoader for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings: Array of shape (num_samples, embedding_dim)\n",
    "    - labels: Array of shape (num_samples, num_labels)\n",
    "    \"\"\"\n",
    "    embeddings, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_masks, batch_labels in tqdm(dataloader, total=len(dataloader)):\n",
    "            batch_x = batch_x.to(\"cpu\").float()\n",
    "            batch_masks = batch_masks.to(\"cpu\")\n",
    "\n",
    "            output = model(x_enc=batch_x, input_mask=batch_masks)  # [batch_size x d_model (=1024)]\n",
    "            embedding = output.embeddings\n",
    "            embeddings.append(embedding.detach().cpu().numpy())\n",
    "            labels.append(batch_labels)\n",
    "\n",
    "    embeddings, labels = np.concatenate(embeddings), np.concatenate(labels)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-moment-load",
   "metadata": {},
   "source": "## Load MOMENT Model and Generate BTS Embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "# Load MOMENT-1-large in embedding mode\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "    \"AutonLab/MOMENT-1-large\", \n",
    "    model_kwargs={'task_name': 'embedding'},\n",
    ")\n",
    "model.init()\n",
    "model.to(\"cpu\").float()\n",
    "\n",
    "# Generate embeddings for training data\n",
    "print(\"Generating training embeddings...\")\n",
    "train_embeddings, train_labels = get_embedding(model, train_dataloader)\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "# Generate embeddings for test data\n",
    "print(\"\\nGenerating test embeddings...\")\n",
    "test_embeddings, test_labels = get_embedding(model, test_dataloader)\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-save",
   "metadata": {},
   "source": "## Save BTS Embeddings and Labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and labels to files\n",
    "np.save(\"train_embeddings.npy\", train_embeddings)\n",
    "np.save(\"train_labels.npy\", train_labels)\n",
    "np.save(\"test_embeddings.npy\", test_embeddings)\n",
    "np.save(\"test_labels.npy\", test_labels)\n",
    "\n",
    "print(\"Embeddings and labels saved successfully!\")\n",
    "print(f\"  - train_embeddings.npy: {train_embeddings.shape}\")\n",
    "print(f\"  - train_labels.npy: {train_labels.shape}\")\n",
    "print(f\"  - test_embeddings.npy: {test_embeddings.shape}\")\n",
    "print(f\"  - test_labels.npy: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates the TSFM embedding generation process for both BTS and WHITED datasets:\n\n### BTS Dataset\n1. Loaded the dataset (31,839 samples with 94 multi-label targets)\n2. Resampled all time series to length 512 using linear interpolation\n3. Split data into 80% train (25,471 samples) and 20% test (6,368 samples)\n4. Generated 1024-dimensional embeddings using MOMENT-1-large foundation model\n5. Saved embeddings to: `train_embeddings.npy`, `test_embeddings.npy`\n\n### WHITED Dataset\n1. Loaded FLAC audio files and extracted instantaneous power (V × I)\n2. Processed 56 appliance types with ~1,339 total samples\n3. Resampled to length 512 and split into 80% train / 20% test\n4. Generated embeddings using two foundation models:\n   - **MOMENT-1-large**: 1024-dimensional embeddings → `moment_train_embeddings_whited.npy`, `moment_test_embeddings_whited.npy`\n   - **Chronos-T5-Large**: Variable-dimensional embeddings → `chronos_train_embeddings_whited.npy`, `chronos_test_embeddings_whited.npy`\n\n**The saved embedding files can be used for any downstream classification algorithm (SVM, Random Forest, Neural Networks, etc.).**\n\n**Note:** For a complete training and evaluation pipeline with SVM classification and metrics reporting, use the Python scripts:\n- `train_tsfm_whited.py --model moment` (MOMENT with SVM)\n- `train_tsfm_whited.py --model chronos` (Chronos with SVM)\n- `train_ts2vec_whited.py` (TS2Vec baseline with SVM)\n- `train_dtw_whited.py` (DTW baseline)\n- `train_resnet_whited.py` (ResNet baseline)"
  },
  {
   "cell_type": "markdown",
   "id": "6zv78y7jg6n",
   "source": "---\n\n# WHITED Dataset - MOMENT Embedding Generation\n\nThe WHITED dataset has a different structure from BTS - it uses FLAC audio files to represent electrical appliance signatures. Below we demonstrate the same MOMENT embedding generation process for WHITED data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "iuw7z8qaeam",
   "source": "## Key Differences: WHITED vs BTS\n\n**WHITED Dataset:**\n- **File Format:** FLAC audio files (2-channel: voltage and current)\n- **Feature Extraction:** Instantaneous power = Voltage × Current\n- **Label Structure:** Single-label classification (56 appliance types)\n- **Dataset Size:** ~1,339 samples (1,071 train / 268 test)\n- **Examples:** Kettle, Toaster, TV, Laptop, Microwave, etc.\n\n**BTS Dataset:**\n- **File Format:** PKL files from ZIP archive\n- **Feature Extraction:** Direct time series values\n- **Label Structure:** Multi-label classification (94 building system labels)\n- **Dataset Size:** ~31,839 samples (25,471 train / 6,368 test)\n- **Examples:** Temperature sensors, flow sensors, pressure sensors, etc.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "olk2zc0pbja",
   "source": "## WHITED Data Loading from FLAC Files",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dtvknch2b34",
   "source": "import soundfile as sf  # To handle FLAC files\n\n# Path to WHITED dataset folder\nwhited_folder_path = \"data/WHITEDv1.1\"\n\ndef process_flac_files(folder_path):\n    \"\"\"\n    Process all FLAC files in a folder and extract instantaneous power.\n    \n    Parameters:\n    - folder_path: Path to folder containing FLAC files\n    \n    Returns:\n    - Dictionary of time series data keyed by appliance type\n    \"\"\"\n    data_dict = {}\n    \n    for file_name in os.listdir(folder_path):\n        if file_name.endswith(\".flac\"):\n            # Extract appliance key from filename (e.g., \"Kettle_001.flac\" -> \"Kettle\")\n            key = file_name.split(\"_\")[0]\n            file_path = os.path.join(folder_path, file_name)\n            \n            # Load FLAC file (2 channels: voltage and current)\n            data, samplerate = sf.read(file_path)\n            \n            # Calculate instantaneous power (V × I)\n            instantaneous_power = data[:, 0] * data[:, 1]\n            \n            # Store in dictionary\n            if key not in data_dict:\n                data_dict[key] = []\n            data_dict[key].append(instantaneous_power)\n    \n    print(f\"Processed {len(data_dict)} unique appliance types.\")\n    for key, timeseries_list in data_dict.items():\n        print(f\"  {key}: {len(timeseries_list)} samples\")\n    \n    return data_dict\n\n# Load WHITED data\nwhited_data_dict = process_flac_files(whited_folder_path)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h6he6xmin4r",
   "source": "## WHITED Data Resampling and Dataset Creation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bpgj90bgfm",
   "source": "def resample_whited_data(data_dict, target_steps=512):\n    \"\"\"\n    Resample all WHITED time series to target length.\n    \n    Parameters:\n    - data_dict: Dictionary of appliance time series\n    - target_steps: Target length for resampling\n    \n    Returns:\n    - Dictionary with resampled time series\n    \"\"\"\n    resampled_dict = {}\n    for key, timeseries_list in data_dict.items():\n        resampled_dict[key] = []\n        for ts in timeseries_list:\n            # Resample using same function as BTS\n            resampled_ts = resample_time_series(ts, target_length=target_steps)\n            resampled_dict[key].append(resampled_ts)\n    return resampled_dict\n\n\nclass WHITEDDatasetWithMask(Dataset):\n    \"\"\"\n    Dataset for WHITED single-label classification with MOMENT model.\n    \"\"\"\n    def __init__(self, resampled_data_dict, seq_len=512, data_split='train', \n                 test_size=0.2, random_state=42):\n        \"\"\"\n        Parameters:\n        - resampled_data_dict: Dictionary with appliance types and time series\n        - seq_len: Fixed sequence length\n        - data_split: 'train' or 'test'\n        - test_size: Proportion for test split\n        - random_state: Random seed\n        \"\"\"\n        self.seq_len = seq_len\n        \n        # Flatten data and create labels\n        all_data = []\n        all_input_masks = []\n        all_labels = []\n        label_mapping = {key: idx for idx, key in enumerate(resampled_data_dict.keys())}\n        \n        print(f\"Number of appliance classes: {len(label_mapping)}\")\n        \n        for key, timeseries_list in resampled_data_dict.items():\n            label = label_mapping[key]\n            for timeseries in timeseries_list:\n                # Flatten if multi-dimensional\n                if timeseries.ndim > 1:\n                    timeseries = timeseries.flatten()\n                \n                timeseries_len = len(timeseries)\n                \n                # Create input mask\n                input_mask = np.ones(seq_len)\n                input_mask[:seq_len - timeseries_len] = 0\n                \n                # Pad time series\n                padded_timeseries = np.pad(timeseries, (seq_len - timeseries_len, 0))\n                \n                # Add channel dimension\n                padded_timeseries = np.expand_dims(padded_timeseries, axis=0)\n                \n                all_data.append(padded_timeseries)\n                all_input_masks.append(input_mask)\n                all_labels.append(label)\n        \n        # Convert to arrays\n        all_data = np.array(all_data)\n        all_input_masks = np.array(all_input_masks)\n        all_labels = np.array(all_labels)\n        \n        # Train/test split\n        train_data, test_data, train_masks, test_masks, train_labels, test_labels = train_test_split(\n            all_data, all_input_masks, all_labels, test_size=test_size, random_state=random_state\n        )\n        \n        print(f\"Train Data Shape: {train_data.shape}\")\n        print(f\"Test Data Shape: {test_data.shape}\")\n        \n        if data_split == 'train':\n            self.data = train_data\n            self.input_masks = train_masks\n            self.labels = train_labels\n        elif data_split == 'test':\n            self.data = test_data\n            self.input_masks = test_masks\n            self.labels = test_labels\n        else:\n            raise ValueError(\"data_split must be 'train' or 'test'\")\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        timeseries = self.data[idx]\n        input_mask = self.input_masks[idx]\n        label = self.labels[idx]\n        return (\n            torch.tensor(timeseries, dtype=torch.float32),\n            torch.tensor(input_mask, dtype=torch.float32),\n            torch.tensor(label, dtype=torch.long),  # Single label (not multi-label)\n        )\n\n\n# Resample WHITED data\nwhited_resampled = resample_whited_data(whited_data_dict, target_steps=512)\n\n# Create datasets\nwhited_train_dataset = WHITEDDatasetWithMask(whited_resampled, data_split='train')\nwhited_test_dataset = WHITEDDatasetWithMask(whited_resampled, data_split='test')\n\n# Create dataloaders\nwhited_train_dataloader = DataLoader(whited_train_dataset, batch_size=64, shuffle=True, drop_last=False)\nwhited_test_dataloader = DataLoader(whited_test_dataset, batch_size=64, shuffle=False, drop_last=False)\n\n# Verify\nfor batch_x, batch_masks, batch_labels in whited_train_dataloader:\n    print(f\"WHITED Batch X Shape: {batch_x.shape}\")\n    print(f\"WHITED Batch Masks Shape: {batch_masks.shape}\")\n    print(f\"WHITED Batch Labels Shape: {batch_labels.shape}\")\n    break",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b1ncog4n69",
   "source": "## Generate WHITED Embeddings with MOMENT\n\nWe use the same MOMENT model and `get_embedding()` function from the BTS section.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x0sxhs3ro8m",
   "source": "# Note: Reuse the same MOMENT model loaded earlier for BTS\n# If not already loaded, uncomment the following:\n# from momentfm import MOMENTPipeline\n# model = MOMENTPipeline.from_pretrained(\n#     \"AutonLab/MOMENT-1-large\", \n#     model_kwargs={'task_name': 'embedding'},\n# )\n# model.init()\n# model.to(\"cpu\").float()\n\n# Generate WHITED embeddings\nprint(\"Generating WHITED training embeddings...\")\nwhited_train_embeddings, whited_train_labels = get_embedding(model, whited_train_dataloader)\nprint(f\"WHITED Train embeddings shape: {whited_train_embeddings.shape}\")\nprint(f\"WHITED Train labels shape: {whited_train_labels.shape}\")\n\nprint(\"\\nGenerating WHITED test embeddings...\")\nwhited_test_embeddings, whited_test_labels = get_embedding(model, whited_test_dataloader)\nprint(f\"WHITED Test embeddings shape: {whited_test_embeddings.shape}\")\nprint(f\"WHITED Test labels shape: {whited_test_labels.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xv5ld1uxk6d",
   "source": "## Save WHITED Embeddings and Labels",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5i8m226zzmg",
   "source": "# Save WHITED embeddings and labels with distinct filenames\nnp.save(\"moment_train_embeddings_whited.npy\", whited_train_embeddings)\nnp.save(\"moment_train_labels_whited.npy\", whited_train_labels)\nnp.save(\"moment_test_embeddings_whited.npy\", whited_test_embeddings)\nnp.save(\"moment_test_labels_whited.npy\", whited_test_labels)\n\nprint(\"WHITED embeddings and labels saved successfully!\")\nprint(f\"  - moment_train_embeddings_whited.npy: {whited_train_embeddings.shape}\")\nprint(f\"  - moment_train_labels_whited.npy: {whited_train_labels.shape}\")\nprint(f\"  - moment_test_embeddings_whited.npy: {whited_test_embeddings.shape}\")\nprint(f\"  - moment_test_labels_whited.npy: {whited_test_labels.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mmq0n9bx6wf",
   "source": "---\n\n## Alternative: Generate WHITED Embeddings with Chronos\n\nChronos is another time series foundation model that can generate embeddings. Below we show how to generate embeddings using Chronos-T5-Large for comparison with MOMENT.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7oav6n7r3o9",
   "source": "from chronos import ChronosPipeline\n\n# Load Chronos-T5-Large model\npipeline = ChronosPipeline.from_pretrained(\n    \"amazon/chronos-t5-large\",\n    device_map=\"cpu\",\n    torch_dtype=torch.bfloat16,\n)\nchronos_model = pipeline.embed",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gunaufjweom",
   "source": "def get_chronos_embedding(model, dataloader):\n    \"\"\"\n    Generates embeddings using Chronos model.\n    \n    Parameters:\n    - model: Chronos embed function (pipeline.embed)\n    - dataloader: DataLoader for the dataset\n    \n    Returns:\n    - embeddings: Array of shape (num_samples, embedding_dim)\n    - labels: Array of labels\n    \"\"\"\n    embeddings, labels = [], []\n    with torch.no_grad():\n        for batch_x, batch_masks, batch_labels in tqdm(dataloader, total=len(dataloader)):\n            batch_x = batch_x.to(\"cpu\").float()\n            \n            # Chronos processes each sample individually\n            embedding = []\n            for b in batch_x:\n                _embedding = model(b[0].cpu())[0]\n                embedding.append(_embedding)\n            \n            embedding = torch.stack(embedding).to(torch.float32)\n            # Average the embedding over sequence length\n            embedding = embedding.mean(dim=2)\n            # Reshape to flatten\n            embedding = embedding.reshape(embedding.shape[0], -1)\n            \n            embeddings.append(embedding.detach().cpu().numpy())\n            labels.append(batch_labels)\n\n    embeddings, labels = np.concatenate(embeddings), np.concatenate(labels)\n    return embeddings, labels\n\n\n# Generate Chronos embeddings for WHITED\nprint(\"Generating WHITED training embeddings with Chronos...\")\nchronos_train_embeddings, chronos_train_labels = get_chronos_embedding(chronos_model, whited_train_dataloader)\nprint(f\"Chronos Train embeddings shape: {chronos_train_embeddings.shape}\")\nprint(f\"Chronos Train labels shape: {chronos_train_labels.shape}\")\n\nprint(\"\\nGenerating WHITED test embeddings with Chronos...\")\nchronos_test_embeddings, chronos_test_labels = get_chronos_embedding(chronos_model, whited_test_dataloader)\nprint(f\"Chronos Test embeddings shape: {chronos_test_embeddings.shape}\")\nprint(f\"Chronos Test labels shape: {chronos_test_labels.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jfwt9tv9gpi",
   "source": "## Save Chronos Embeddings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kcyvh7k239g",
   "source": "# Save Chronos embeddings and labels with distinct filenames\nnp.save(\"chronos_train_embeddings_whited.npy\", chronos_train_embeddings)\nnp.save(\"chronos_train_labels_whited.npy\", chronos_train_labels)\nnp.save(\"chronos_test_embeddings_whited.npy\", chronos_test_embeddings)\nnp.save(\"chronos_test_labels_whited.npy\", chronos_test_labels)\n\nprint(\"Chronos embeddings and labels saved successfully!\")\nprint(f\"  - chronos_train_embeddings_whited.npy: {chronos_train_embeddings.shape}\")\nprint(f\"  - chronos_train_labels_whited.npy: {chronos_train_labels.shape}\")\nprint(f\"  - chronos_test_embeddings_whited.npy: {chronos_test_embeddings.shape}\")\nprint(f\"  - chronos_test_labels_whited.npy: {chronos_test_labels.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2ewoj1oj3ge",
   "source": "## Note: Using train_tsfm_whited.py for End-to-End Workflow\n\nThis notebook focuses on **embedding generation only**. If you want a complete end-to-end workflow that includes:\n- Embedding generation\n- SVM classifier training\n- Evaluation metrics (accuracy, precision, recall, F1-score)\n- Results saving\n\nUse the Python script instead:\n```bash\n# For MOMENT embeddings + SVM classifier\npython train_tsfm_whited.py --model moment\n\n# For Chronos embeddings + SVM classifier\npython train_tsfm_whited.py --model chronos\n```\n\nThe script `train_tsfm_whited.py` implements the same embedding generation process shown in this notebook, plus automatic SVM training and comprehensive evaluation.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}